{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import esri_retinanet_config as config\n",
    "from bs4 import BeautifulSoup\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"./retinanet/\")\n",
    "files = glob.glob('./dataset/images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    " \n",
    "files = glob.glob('./dataset/annotations/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "case = \"rgb\"\n",
    "lbls = \"taxonID\"\n",
    "sites = [\"OSBS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset dataset folder and include only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_f = (glob.glob('../'+case+'/*.tif'))\n",
    "for f in ls_f:\n",
    "    shutil.copy(f, './dataset/images/')\n",
    "    \n",
    "ls_f = (glob.glob('../csv_labels/*'+case+'.csv'))\n",
    "for f in ls_f:\n",
    "    shutil.copy(f, './dataset/annotations/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the base path for the logos dataset\n",
    "BASE_PATH = \"dataset\"\n",
    "\n",
    "# build the path to the annotations and input images\n",
    "annot_path = os.path.sep.join([BASE_PATH, 'annotations'])\n",
    "images_path = os.path.sep.join([BASE_PATH, 'images'])\n",
    "\n",
    "# define the training/testing split\n",
    "train_test_split = 0.8\n",
    "\n",
    "#  build the path to the output training and test .csv files\n",
    "train_csv = os.path.sep.join([BASE_PATH, 'train.csv'])\n",
    "test_csv = os.path.sep.join([BASE_PATH, 'test.csv'])\n",
    "\n",
    "# build the path to the output classes CSV files\n",
    "classes_csv = os.path.sep.join([BASE_PATH, 'classes.csv'])\n",
    "\n",
    "# build the path to the output predictions dir\n",
    "OUTPUT_DIR = os.path.sep.join([BASE_PATH, 'predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all image paths then construct the training and testing split\n",
    "imagePaths = list(paths.list_files(images_path))\n",
    "random.seed( 30 )\n",
    "random.shuffle(imagePaths)\n",
    "i = int(len(imagePaths) * train_test_split)\n",
    "trainImagePaths = imagePaths[:i]\n",
    "testImagePaths = imagePaths[i:]\n",
    "\n",
    "# create the list of datasets to build\n",
    "dataset = [ (\"train\", trainImagePaths, train_csv),\n",
    "            (\"test\", testImagePaths, test_csv)]\n",
    "\n",
    "# initialize the set of classes we have\n",
    "CLASSES = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sergiomarconi/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "veg_structure = pd.read_csv(\"vegetation_structure.csv\")\n",
    "veg_structure = veg_structure[[\"individualID\", \"siteID\", lbls]]\n",
    "#is_sites = veg_structure.siteID.isin(list(sites))\n",
    "veg_structure = veg_structure[veg_structure.siteID.isin(list(sites))]\n",
    "list_labels = veg_structure[lbls].unique()\n",
    "lb_code = range(len(list_labels)+1)\n",
    "labelnet = {}\n",
    "for i in range(len(list_labels)):\n",
    "    labelnet[list_labels[i]] = lb_code[i+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QULA2': 1,\n",
       " 'PIPA2': 2,\n",
       " 'QUGE2': 3,\n",
       " 'DIOSP': 4,\n",
       " 'PINUS': 5,\n",
       " 'QUNI': 6,\n",
       " 'ACRU': 7,\n",
       " 'ILCA': 8,\n",
       " 'MOCE2': 9,\n",
       " 'PEPA37': 10,\n",
       " 'GOLA': 11,\n",
       " 'LYLU3': 12,\n",
       " 'PIEL': 13,\n",
       " 'NYBI': 14,\n",
       " 'QUHE2': 15,\n",
       " 'CEOC': 16,\n",
       " 'ILMY': 17,\n",
       " 'PITA': 18,\n",
       " 'QULA3': 19,\n",
       " 'QUIN': 20,\n",
       " 'QUVI': 21,\n",
       " 'VIRO3': 22,\n",
       " 'ASIN12': 23,\n",
       " 'QUMA13': 24,\n",
       " 'VAAR': 25,\n",
       " 'SAET': 26}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelnet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels\n",
    "labels = labels.rename(columns={'label': 'individualID'})\n",
    "labels = labels.join(veg_structure.set_index('individualID'), on='individualID')\n",
    "labels=labels.replace({\"taxonID\": labelnet})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] creating 'train' set...\n",
      "[INFO] 32 total images in 'train' set\n",
      "[INFO] creating 'test' set...\n",
      "[INFO] 9 total images in 'test' set\n"
     ]
    }
   ],
   "source": [
    "for (dType, imagePaths, outputCSV) in dataset:\n",
    "    # load the contents\n",
    "    print (\"[INFO] creating '{}' set...\".format(dType))\n",
    "    print (\"[INFO] {} total images in '{}' set\".format(len(imagePaths), dType))\n",
    "\n",
    "    # open the output CSV file\n",
    "    #csv = open(outputCSV, \"w\")\n",
    "\n",
    "    # loop over the image paths\n",
    "    labels = pd.DataFrame()\n",
    "    for imagePath in imagePaths:\n",
    "        fl = imagePath.split(\"/\")[2][:-4]\n",
    "        tmp = pd.read_csv('./dataset/annotations/'+fl+'.csv')\n",
    "        labels = labels.append(tmp, sort=False)\n",
    "        \n",
    "    labels = labels.rename(columns={'label': 'individualID'})\n",
    "    labels = labels.join(veg_structure.set_index('individualID'), on='individualID')\n",
    "    labels=labels.replace({lbls: labelnet})\n",
    "    labels = labels.rename(columns={lbls: 'label'})\n",
    "\n",
    "\n",
    "    labels.to_csv(\"./\"+outputCSV)\n",
    "    # update the set of unique class labels\n",
    "    #CLASSES.add(label)\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# loop over the datasets\n",
    "for (dType, imagePaths, outputCSV) in dataset:\n",
    "    # load the contents\n",
    "    print (\"[INFO] creating '{}' set...\".format(dType))\n",
    "    print (\"[INFO] {} total images in '{}' set\".format(len(imagePaths), dType))\n",
    "\n",
    "    # open the output CSV file\n",
    "    csv = open(outputCSV, \"w\")\n",
    "    \n",
    "    # loop over the image paths\n",
    "    for imagePath in imagePaths:\n",
    "        # build the corresponding annotation path\n",
    "        fname = imagePath.split(os.path.sep)[-1]\n",
    "        fname = \"{}.xml\".format(fname[:fname.rfind(\".\")])\n",
    "        annotPath = os.path.sep.join([annot_path, fname])\n",
    "\n",
    "        # load the contents of the annotation file and buid the soup\n",
    "        contents = open(annotPath).read()\n",
    "        soup = BeautifulSoup(contents, \"html.parser\")\n",
    "\n",
    "        # extract the image dimensions\n",
    "        w = int(width/xres)\n",
    "        h = int(height/yres)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# loop over all object elements\n",
    "        for o in soup.find_all(\"object\"):\n",
    "            #extract the label and bounding box coordinates\n",
    "            label = o.find(\"taxonID\").string\n",
    "            xMin = int(float(o.find(\"xmin\").string))\n",
    "            yMin = int(float(o.find(\"ymin\").string))\n",
    "            xMax = int(float(o.find(\"xmax\").string))\n",
    "            yMax = int(float(o.find(\"ymax\").string))\n",
    "\n",
    "            # truncate any bounding box coordinates that fall outside\n",
    "            # the boundaries of the image\n",
    "            xMin = max(0, xMin)\n",
    "            yMin = max(0, yMin)\n",
    "            xMax = min(w, xMax)\n",
    "            yMax = min(h, yMax)\n",
    "\n",
    "            # ignore the bounding boxes where the minimum values are larger\n",
    "            # than the maximum values and vice-versa due to annotation errors\n",
    "            if xMin >= xMax or yMin >= yMax:\n",
    "                continue\n",
    "            elif xMax <= xMin or yMax <= yMin:\n",
    "                continue\n",
    "\n",
    "            # write the image path, bb coordinates, label to the output CSV\n",
    "            row = [os.path.abspath(imagePath),str(xMin), str(yMin), str(xMax),\n",
    "                    str(yMax), str(label)]\n",
    "            csv.write(\"{}\\n\".format(\",\".join(row)))\n",
    "\n",
    "            # update the set of unique class labels\n",
    "            CLASSES.add(label)\n",
    "\n",
    "    # close the CSV file\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write the classes to file\n",
    "print(\"[INFO] writing classes...\")\n",
    "csv = open(classes_csv, \"w\")\n",
    "rows = [\",\".join([c, str(i)]) for (i,c) in enumerate(CLASSES)]\n",
    "csv.write(\"\\n\".join(rows))\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.from_dict(labelnet)\n",
    "new_dict = {k: [v] for k, v in labelnet.items()}\n",
    "df = pd.DataFrame(new_dict)\n",
    "df = df.T\n",
    "df.to_csv(\"./species_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0  index\n",
      "QULA2    1      0\n",
      "PIPA2    2      1\n",
      "QUGE2    3      2\n",
      "DIOSP    4      3\n",
      "PINUS    5      4\n",
      "QUNI     6      5\n",
      "ACRU     7      6\n",
      "ILCA     8      7\n",
      "MOCE2    9      8\n",
      "PEPA37  10      9\n",
      "GOLA    11     10\n",
      "LYLU3   12     11\n",
      "PIEL    13     12\n",
      "NYBI    14     13\n",
      "QUHE2   15     14\n",
      "CEOC    16     15\n",
      "ILMY    17     16\n",
      "PITA    18     17\n",
      "QULA3   19     18\n",
      "QUIN    20     19\n",
      "QUVI    21     20\n",
      "VIRO3   22     21\n",
      "ASIN12  23     22\n",
      "QUMA13  24     23\n",
      "VAAR    25     24\n",
      "SAET    26     25\n"
     ]
    }
   ],
   "source": [
    "df['index'] = list(range(len(df)))\n",
    "print(df)\n",
    "df.to_csv(\"./\"+classes_csv, header=False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
